{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsGMo9LvmXdAuvIXZdhmgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megh-Zyke/Neural-Network-models/blob/main/functions_definations/Planar_Data_Classification_using_Shallow_neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNgPDV0BWNzj",
        "outputId": "7390b87b-8f55-4d1a-9ad9-9a0783376a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Non Linear Activation Functions\n",
        "\n",
        "The non linear activation functions used in Neural Network models are :\n",
        "\n",
        "- Sigmoid Function\n",
        "\n",
        "  $S(x)= \\frac {1}{1+e^{-x}}$\n",
        "\n",
        "- Hyperbolic Tangent Function (tanh(x))\n",
        "\n",
        "  $tanh(x) = \\frac {e^x â€“ e^{-x}} {e^x + e^{-x}}$\n",
        "- Rectified Linear Unit Function (ReLu(x))\n",
        "\n",
        "  $ReLu(x)  = max{(0, z)}$\n"
      ],
      "metadata": {
        "id": "yFKvOuSVfemB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  denominator  = 1 + np.exp(-x)\n",
        "  sigmoid = 1/denominator\n",
        "\n",
        "  return sigmoid"
      ],
      "metadata": {
        "id": "XhwW6xNfhNKF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "\n",
        "  num =np.exp(x) + np.exp(-x)\n",
        "  den = np.exp(x) - np.exp(-x)\n",
        "\n",
        "  tanh = num/den\n",
        "\n",
        "  return tanh"
      ],
      "metadata": {
        "id": "ZL1lI9ARhZsg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "\n",
        "\n",
        "    W1 = np.random.randn(n_h,n_x)*0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h)*0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "x4CVb6CQk0hF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forward Propagation\n",
        "\n",
        "Implement `forward_propagation()` using the following equations:\n",
        "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$\n",
        "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
        "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "sG-ciseedPaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X,parameters):\n",
        "\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "    Z1 = np.dot(W1,X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2,A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "\n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache"
      ],
      "metadata": {
        "id": "Byg7x0qoX9D3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing Cost Function\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
        "\n",
        "\n",
        "The above given mathematical function is the `Cost Function` of the Neural Network. The cost function helps us average out the entire losses of the dataset.\n",
        "\n",
        "It is the average of the loss function used in this Neural Network model.\n",
        "\n",
        "`Loss Function : `\n",
        "\n",
        "$$L =  \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$"
      ],
      "metadata": {
        "id": "R1JjZO0qd6lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computing_cost(A2,Y):\n",
        "  m = A2.shape[0]\n",
        "  loss_function = np.sum(Y*np.log(A2)) + np.sum((1-Y)*np.log(1-A2))\n",
        "\n",
        "  J = -1*np.sum(loss_function)/m\n",
        "  cost = float(np.squeeze(J))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "mhLdQZ21d1Pe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of Backward Propagation\n",
        "\n",
        "Backpropagation is one of the most important steps in Neural Network Building as it in this level that weights and biases are updated to reduce the cost function.\n",
        "\n",
        "We find the derivative of the cost function with respect to each of the weights and biases and then we apply gradient descent on the function to minimize the value of the cost function\n",
        "\n",
        "`Cost Function:`$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$\n",
        "\n",
        "\n",
        "`Derivatives:`\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n"
      ],
      "metadata": {
        "id": "3DqUa4ZShtkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "\n",
        "\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1/m) * np.dot(dZ2,A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1,2))\n",
        "    dW1 = (1/m) * np.dot(dZ1,X.T)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "iVvEyYVQfcqO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent\n",
        "\n",
        "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
        "\n",
        "$\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$"
      ],
      "metadata": {
        "id": "r7XGgYUwjiHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(parameters, grads, learning_rate = 1.2):\n",
        "\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "7cydCjsgjXSI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integration of functions in `nn_model()`\n",
        "\n",
        "Building your neural network model in `nn_model()`."
      ],
      "metadata": {
        "id": "KGTIU1QZkHrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "\n",
        "\n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "\n",
        "\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        cost = computing_cost(A2,Y)\n",
        "\n",
        "        grads =  backward_propagation(parameters, cache, X, Y)\n",
        "        parameters =  gradient_descent(parameters, grads)\n",
        "\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "6up67yKfj4SV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyVWOq5sk6E2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}